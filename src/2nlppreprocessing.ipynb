{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Library ini akan digunakan untuk melakukan pemrosesan text bahasa indonesia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Data yang akan digunakan merupakan data twitter yang sudah dikumpulkan sebelumnya dan disimpan ke dalam format `csv`. Data ini di load menggunakan library `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data dari file csv\n",
    "data = pd.read_csv('../data/mini_dataset.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada data ini terdapat 5 kolom yaitu:\n",
    "\n",
    "| Nama Kolom | Keterangan |\n",
    "| ---------- | ---------- | \n",
    "|**`Id`** | Id data tweet |\n",
    "|**`Sentiment`** | Label sentiment untuk training data |\n",
    "|**`Jumlah Retweet`** | Jumlah data retweet per tweet |\n",
    "|**`Text Tweet`** | Data text tweet yang diambil |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melihat data pada index 0\n",
    "data['Text Tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextTweet = data['Text Tweet']\n",
    "\n",
    "TextTweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URL, Hashtag, Mention Emoji\n",
    "\n",
    "Informasi URL, hastag, dan emoji merupakan informasi yang tidak banyak digunakan di dalam analsis. Sehingga dapat dihapus dari text yang kita miliki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library yang digunakan untuk cleansing\n",
    "import preprocessor as p\n",
    "\n",
    "# membuat list kosong untuk menyimpan hasil\n",
    "clean_text = []\n",
    "\n",
    "# membuat for loop untuk melakukan cleansing pada setiap elemen\n",
    "for text in TextTweet:\n",
    "    clean_text.append(p.clean(text))\n",
    "\n",
    "# melihat hasil cleansing\n",
    "clean_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercasing\n",
    "\n",
    "Dalam analisis data, besar kecil merupakan hal yang sensitif, sehingga penting untuk melakukan penyeragaman pada besar kecil huruf data text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat list kosong untuk menyimpan hasil \n",
    "lower_text = []\n",
    "\n",
    "# membuat loop untuk melakukan penyeragaman elemen\n",
    "for text in clean_text:\n",
    "    lower_text.append(text.lower())\n",
    "\n",
    "# melihat hasil cleansing\n",
    "lower_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stopwords\n",
    "\n",
    "Tidak semua kata memiliki makna, pada kata yang tidak memiliki makna berarti dapat dihilangkan untuk dapat membuat analisis menjadi lebih sederhana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library untuk menghilangkan stopword\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, ArrayDictionary, StopWordRemover\n",
    "\n",
    "# membuat object untuk menghilankan stopwords\n",
    "stop_factory = StopWordRemoverFactory()\n",
    "\n",
    "# melihat kata-kata yang terdapat pada stopwords\n",
    "print(stop_factory.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat objek stopwords\n",
    "stopword = stop_factory.create_stop_word_remover()\n",
    "\n",
    "# membuat list kosong untuk menyimpan hasil\n",
    "no_stopwords_text = []\n",
    "\n",
    "# membuit loop untuk menghilangkan stopwords\n",
    "for text in lower_text:\n",
    "    no_stopwords_text.append(stopword.remove(text))\n",
    "\n",
    "# melihat hasil\n",
    "no_stopwords_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika ada kata-kata yang secara custom mau dimasukkan bisa menambahkan ke dalam dictionary secara manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menambahkan kata yang ingin dihilangkan\n",
    "more_stopwords = ['undang']\n",
    "\n",
    "# membuat stopwords yang digabungkan dengan dictionary yang sudah ada\n",
    "data = stop_factory.get_stop_words() + more_stopwords\n",
    "dictionary_stop = ArrayDictionary(data)\n",
    "str = StopWordRemover(dictionary_stop)\n",
    "\n",
    "# melihat hasil\n",
    "print(str.remove(lower_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import untuk ReGex\n",
    "import re\n",
    "\n",
    "# membuat fungsi regex untuk menhapus tanda baca\n",
    "def remove_punct(text):\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct(\"?><:<:<:sekarang ha@ri se^^nin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat list kosong untuk menyimpan hasil\n",
    "no_punct_text=[]\n",
    "\n",
    "# membuat loop untuk menghapus tanda pada baca pada setiap elemen\n",
    "for text in no_stopwords_text:\n",
    "    no_punct_text.append(remove_punct(text))\n",
    "\n",
    "# melihat hasil\n",
    "no_punct_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Kata dengan imbuhan menambah kompleksitas pada analisis data, terutama jika menggunakan fitur yang perlu melakukan word count. Hal ini dapat diatasi dengan mengembalikan kata ke dalam bentuk kata dasar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library untuk mengembalikan ke dalam bentuk kata dasar\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# membuat fungsi untuk stemming\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# contoh menggunakan stemming\n",
    "sentence = 'Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan dimana pempek adaan adalah barang export nomor 1'\n",
    "output = stemmer.stem(sentence)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat list kosong untuk menyimpan hasil\n",
    "stemmed_text = []\n",
    "\n",
    "# melakukan looping untuk melakukan stemming pada setiap elemen\n",
    "for text in no_punct_text:\n",
    "    stemmed_text.append(stemmer.stem(text))\n",
    "\n",
    "# melihat hasil stemming\n",
    "stemmed_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "Unit data yang text yang diperoleh biasanya di tingkat paragraf atau kalimat. Unit level tersebut terlalu besar untuk dilakukan analisis sehingga perlu di pisahkan berdasarkan spasi, hingga dapat dilakukan analisis pada tingkat kata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat list kosong untuk menyimpan hasil\n",
    "token = []\n",
    "\n",
    "# membuat loop untuk melakukan tokenisasi pada setiap elemen\n",
    "for text in stemmed_text:\n",
    "    token.append(text.split())\n",
    "\n",
    "# melihat hasil tokenisasi\n",
    "token[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('modelling')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3152bca2417f63d8c485a5409aeed29ee044bafe7ca7cac97f729f4661248f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
